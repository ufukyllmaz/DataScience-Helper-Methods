{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = min(1000000, data['EVENT_COLUMN'].value_counts().min()) # For dividing your data into desired number of elements for each group elemet\n",
    "#data = data.groupby('EVENT_COLUMN').apply(lambda x: x.sample(n=n, random_state=1))\n",
    "data = data.groupby('EVENT_COLUMN').apply(lambda x: x.sample(frac=0.2, random_state=1))\n",
    "data = data.droplevel(0)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting(model, test_data, pred_duration, conditional_after=True):\n",
    "    if conditional_after:\n",
    "        pred_df, upper_ci, lower_ci = model.predict(test_data, return_ci = True)\n",
    "        pred_df['PRED_DURATION'] = pred_duration\n",
    "        pred_df['PRED_DURATION'].loc[pred_df['PRED_DURATION'] < 0] = 1\n",
    "        \n",
    "        for idx, row in pred_df.iterrows():\n",
    "            duration = row['PRED_DURATION']\n",
    "            pred_df.loc[idx] = row / row[duration]\n",
    "        \n",
    "        pred_df = pred_df.drop('PRED_DURATION', axis=1) # in order not to make PRED_DURATION column 1.\n",
    "        pred_df[pred_df > 1.0] = 1 #if there is some values less than '1.0' (it may be 0 or -1).\n",
    "        \n",
    "    else:\n",
    "        pred_df, upper_ci, lower_ci = model.predict(test_data, return_ci = True)\n",
    "    return pred_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_custom_logger(project_folder=\"logs\"):\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates a logger object with rotating file handler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logger : logging.Logger\n",
    "        logger object\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder path and file path\n",
    "    base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "    folder_path = os.path.join(base_dir, project_folder)\n",
    "    file_path = os.path.join(folder_path, \"analytic.log\")\n",
    "\n",
    "    # create folder if not exists\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # create logger object\n",
    "    logger = logging.getLogger(project_folder)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # create formatter\n",
    "    formatter = logging.Formatter(fmt=\"{asctime} {levelname:5} {filename}:{funcName}:{lineno} - {message}\", style=\"{\")\n",
    "    \n",
    "    # create rotating file handler\n",
    "    rotating_file_handler = TimedRotatingFileHandler(filename=file_path, when='D', interval=30, backupCount=6)\n",
    "    rotating_file_handler.setFormatter(formatter)\n",
    "\n",
    "    # add rotating file handler to logger\n",
    "    logger.addHandler(rotating_file_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_custom_logger(project_folder=\"logs\") # define it before usage\n",
    "# then use it with calling logger\n",
    "logger.info(\"Execution Process Started\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(df, chunk_size = 10000): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size else 0)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks\n",
    "#pd.concat(chunks, ignore_index=True) # if you want to concat chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Apply on Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(total_bill, tip):\n",
    "    if tip/total_bill > 0.25:\n",
    "        return 'Generous'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tip Quality'] = df[['total_bill', 'tip']].apply(lambda df: quality(df['total_bill'], df['tip']), axis=1)\n",
    "\n",
    "# OR\n",
    "\n",
    "df['Tip Quality'] = np.vectorize(quality)(df['total_bill'], df['tip'])\n",
    "\n",
    "# Both do the same, but np.vectorize is more faster though np.vectorize is not bult for performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Code Performance Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "setup = \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "def quality(total_bill, tip):\n",
    "    if tip/total_bill > 0.25:\n",
    "        return 'Generous'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\"\"\"\n",
    "stmt_one = \"\"\"\n",
    "df['Tip Quality'] = df[['total_bill', 'tip']].apply(lambda df: quality(df['total_bill'], df['tip']), axis=1)\n",
    "\"\"\"\n",
    "\n",
    "stmt_two = \"\"\"\n",
    "df['Tip Quality'] = np.vectorize(quality)(df['total_bill'], df['tip'])\n",
    "\"\"\"\n",
    "timeit.timeit(setup=setup, stmt=stmt_one, number=100)\n",
    "timeit.timeit(setup=setup, stmt=stmt_two, number=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "df['total_bill'].describe().apply(lambda x: format(x, 'f'))\n",
    "\n",
    "# max() Index Location\n",
    "df['total_bill'].idxmax()\n",
    "df.iloc[df['total_bill'].idxmax()]\n",
    "\n",
    "# min() Index Location\n",
    "df['total_bill'].idxmin()\n",
    "df.iloc[df['total_bill'].idxmin()]\n",
    "\n",
    "# Multiple Replace\n",
    "df['sex'].replace(['Female', 'Male'], ['F', 'M'])\n",
    "#or\n",
    "dictmap = {'Female' : 'F', 'Male' : 'M'}\n",
    "df['sex'].map(dictmap)\n",
    "\n",
    "# Between Method\n",
    "df[df['total_bill'].between(10, 20, inclusive=True)]\n",
    "\n",
    "# nlargest/nsmallest\n",
    "df.nlargest(8, 'tip')    |   df.sort_values('tip', ascending=False).iloc[0:8] # Both give the same output but nlargest is more powerfull\n",
    "\n",
    "df.nsmallest(8, 'tip')    |   df.sort_values('tip', ascending=True).iloc[0:8] # Both give the same output but nsmallest is more powerfull\n",
    "\n",
    "# dropna\n",
    "df.dropna(thresh=3) # gives the rows that have at least 3 notnull columns\n",
    "df.dropna(subset=['last_name']) # onyl dropna of last_name column\n",
    "\n",
    "# groupby\n",
    "df.groupby('model_year').describe() # gives describe of all columns according to model_year\n",
    "\n",
    "year_cyl = df.groupby(['model_year', 'cylinders']).mean()\n",
    "year_cyl.index.names # gives the names (['model_year', 'cylinders'])\n",
    "year_cyl.index.levels # gives the values of above groups [[70,71,72,73], [2,3,4,5,6]]\n",
    "year_cyl.loc[[70,80]] # gives the values of model_year groups 70 and 73\n",
    "year_cyl.xs(key=70, level='model_year') # gives all values of group model_year=70\n",
    "year_cyl.xs(key=5, level='cylinders') # gives all values of group cylinders=5\n",
    "year_cyl.swaplevel() # gives each level\n",
    "\n",
    "# merge\n",
    "pd.merge(registar, login, how='inner', on='name', suffixes = ('_reg', '_log')) \n",
    "# suffixes use the change column name if both dataset has the same column name of columns\n",
    "# like: registar has 'name', 'id' and login has 'name', 'id' --> merge dataset has 'name', 'id_reg', 'id_log'\n",
    "\n",
    "# datetime\n",
    "euro_date = '10-12-2000'    # 10december2000\n",
    "pd.to_datetime(euro_date)   # gives 2000-10-12 means 12october2000 (makes it american datetime)\n",
    "pd.to_datetime(euro_date, dayfirst=True) # gives 2000-12-10 means 10december2000\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['column_name'].value_counts().plot(kind='pie',\n",
    "                                        figsize=(15,8),\n",
    "                                        autopct='%1.0f%%',\n",
    "                                        explode=[0.04, 0.04, 0.04, 0.04, 0.04], # write it as many as distinct elemts are\n",
    "                                        colors=['ping', 'tomato', 'cornflowerblue', 'orange', 'orchid'],\n",
    "                                        shadow=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cfb22f7ad758691768dcc572bcc29c8b09a8a49fcfaf5a8015c4aba90c19555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
