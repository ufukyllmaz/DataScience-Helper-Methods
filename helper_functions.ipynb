{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FillNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values with mode of the column\n",
    "def replace_nan(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].isna().sum() > 0:\n",
    "            df[column] = df[column].fillna(df[column].mode()[0])\n",
    "replace_nan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "sns.boxplot(data=data.iloc[:,1:], orient=\"h\", palette=\"Set2\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quart1, quart2 = data['MonthlyMinutes'].quantile([0.25,0.75])\n",
    "iqr = quart2 - quart1\n",
    "\n",
    "lowerBound = quart1 - (1.5*iqr)\n",
    "upperBound = quart2 + (8*iqr)\n",
    "\n",
    "data['Anomalies'] = ((data['MonthlyMinutes']>upperBound) | (data['MonthlyMinutes']<lowerBound).astype('int'))\n",
    "\n",
    "anomaly = data[data['Anomalies'] == 1]\n",
    "_ = plt.figure(figsize=(15,5))\n",
    "_ = plt.plot(data['MonthlyMinutes'], color='blue', label='Normal')\n",
    "_ = plt.plot(anomaly['MonthlyMinutes'], linestyle='none', marker='X', color='red', label='Anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation control\n",
    "data.corr().style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying columns have greater than 0.80 correlation with some other columns\n",
    "corr = data_1.corr().abs()\n",
    "upper_tri = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))\n",
    "corr_columns = [column for column in upper_tri.columns if any(upper_tri[column] > 0.80)]\n",
    "upper_tri[corr_columns][upper_tri[corr_columns] > 0.80]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = min(1000000, data['EVENT_COLUMN'].value_counts().min()) # For dividing your data into desired number of elements for each group elemet\n",
    "#data = data.groupby('EVENT_COLUMN').apply(lambda x: x.sample(n=n, random_state=1))\n",
    "data = data.groupby('EVENT_COLUMN').apply(lambda x: x.sample(frac=0.2, random_state=1))\n",
    "data = data.droplevel(0)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting(model, test_data, pred_duration, conditional_after=True):\n",
    "    if conditional_after:\n",
    "        pred_df, upper_ci, lower_ci = model.predict(test_data, return_ci = True)\n",
    "        pred_df['PRED_DURATION'] = pred_duration\n",
    "        pred_df['PRED_DURATION'].loc[pred_df['PRED_DURATION'] < 0] = 1\n",
    "        \n",
    "        for idx, row in pred_df.iterrows():\n",
    "            duration = row['PRED_DURATION']\n",
    "            pred_df.loc[idx] = row / row[duration]\n",
    "        \n",
    "        pred_df = pred_df.drop('PRED_DURATION', axis=1) # in order not to make PRED_DURATION column 1.\n",
    "        pred_df[pred_df > 1.0] = 1 #if there is some values less than '1.0' (it may be 0 or -1).\n",
    "        \n",
    "    else:\n",
    "        pred_df, upper_ci, lower_ci = model.predict(test_data, return_ci = True)\n",
    "    return pred_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_custom_logger(project_folder=\"logs\"):\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates a logger object with rotating file handler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    project_folder : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    logger : logging.Logger\n",
    "        logger object\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder path and file path\n",
    "    base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "    folder_path = os.path.join(base_dir, project_folder)\n",
    "    file_path = os.path.join(folder_path, \"analytic.log\")\n",
    "\n",
    "    # create folder if not exists\n",
    "    Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # create logger object\n",
    "    logger = logging.getLogger(project_folder)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # create formatter\n",
    "    formatter = logging.Formatter(fmt=\"{asctime} {levelname:5} {filename}:{funcName}:{lineno} - {message}\", style=\"{\")\n",
    "    \n",
    "    # create rotating file handler\n",
    "    rotating_file_handler = TimedRotatingFileHandler(filename=file_path, when='D', interval=30, backupCount=6)\n",
    "    rotating_file_handler.setFormatter(formatter)\n",
    "\n",
    "    # add rotating file handler to logger\n",
    "    logger.addHandler(rotating_file_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_custom_logger(project_folder=\"logs\") # define it before usage\n",
    "# then use it with calling logger\n",
    "logger.info(\"Execution Process Started\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(df, chunk_size = 10000): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size else 0)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks\n",
    "#pd.concat(chunks, ignore_index=True) # if you want to concat chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Apply on Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(total_bill, tip):\n",
    "    if tip/total_bill > 0.25:\n",
    "        return 'Generous'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tip Quality'] = df[['total_bill', 'tip']].apply(lambda df: quality(df['total_bill'], df['tip']), axis=1)\n",
    "\n",
    "# OR\n",
    "\n",
    "df['Tip Quality'] = np.vectorize(quality)(df['total_bill'], df['tip'])\n",
    "\n",
    "# Both do the same, but np.vectorize is more faster though np.vectorize is not bult for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Apply with args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_manipulation(row, pred_duration):\n",
    "    row= row.tolist()\n",
    "    new_row = []\n",
    "    for prob in row:\n",
    "        if prob != 1.0:\n",
    "            new_row.append(prob)\n",
    "    if len(new_row) > pred_duration:\n",
    "        append_count = pred_duration - len(new_row)\n",
    "        for i in range(append_count):\n",
    "            new_row.append(0)\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swifter is used to perform faster\n",
    "# args needs to get tuple, in this case funciton has one extra parameter which is pred_duration and we need to give it when calling the function.\n",
    "#   So, args = (parameter,) -> this is a tuple with one parameter and yes comma (,) is needed.\n",
    "pred_df = pd.DataFrame(data=pred_df.swifter.apply(pred_manipulation, axis=1, args=(len(pred_df.columns),).tolist()),\n",
    "                       columns=pred_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Code Performance Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "setup = \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "def quality(total_bill, tip):\n",
    "    if tip/total_bill > 0.25:\n",
    "        return 'Generous'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\"\"\"\n",
    "stmt_one = \"\"\"\n",
    "df['Tip Quality'] = df[['total_bill', 'tip']].apply(lambda df: quality(df['total_bill'], df['tip']), axis=1)\n",
    "\"\"\"\n",
    "\n",
    "stmt_two = \"\"\"\n",
    "df['Tip Quality'] = np.vectorize(quality)(df['total_bill'], df['tip'])\n",
    "\"\"\"\n",
    "timeit.timeit(setup=setup, stmt=stmt_one, number=100)\n",
    "timeit.timeit(setup=setup, stmt=stmt_two, number=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "df['total_bill'].describe().apply(lambda x: format(x, 'f'))\n",
    "\n",
    "# max() Index Location\n",
    "df['total_bill'].idxmax()\n",
    "df.iloc[df['total_bill'].idxmax()]\n",
    "\n",
    "# min() Index Location\n",
    "df['total_bill'].idxmin()\n",
    "df.iloc[df['total_bill'].idxmin()]\n",
    "\n",
    "# Multiple Replace\n",
    "df['sex'].replace(['Female', 'Male'], ['F', 'M'])\n",
    "#or\n",
    "dictmap = {'Female' : 'F', 'Male' : 'M'}\n",
    "df['sex'].map(dictmap)\n",
    "\n",
    "# Between Method\n",
    "df[df['total_bill'].between(10, 20, inclusive=True)]\n",
    "\n",
    "# nlargest/nsmallest\n",
    "df.nlargest(8, 'tip')    |   df.sort_values('tip', ascending=False).iloc[0:8] # Both give the same output but nlargest is more powerfull\n",
    "\n",
    "df.nsmallest(8, 'tip')    |   df.sort_values('tip', ascending=True).iloc[0:8] # Both give the same output but nsmallest is more powerfull\n",
    "\n",
    "# dropna\n",
    "df.dropna(thresh=3) # gives the rows that have at least 3 notnull columns\n",
    "df.dropna(subset=['last_name']) # onyl dropna of last_name column\n",
    "\n",
    "# groupby\n",
    "df.groupby('model_year').describe() # gives describe of all columns according to model_year\n",
    "\n",
    "year_cyl = df.groupby(['model_year', 'cylinders']).mean()\n",
    "year_cyl.index.names # gives the names (['model_year', 'cylinders'])\n",
    "year_cyl.index.levels # gives the values of above groups [[70,71,72,73], [2,3,4,5,6]]\n",
    "year_cyl.loc[[70,80]] # gives the values of model_year groups 70 and 73\n",
    "year_cyl.xs(key=70, level='model_year') # gives all values of group model_year=70\n",
    "year_cyl.xs(key=5, level='cylinders') # gives all values of group cylinders=5\n",
    "year_cyl.swaplevel() # gives each level\n",
    "\n",
    "# merge\n",
    "pd.merge(registar, login, how='inner', on='name', suffixes = ('_reg', '_log')) \n",
    "# suffixes use the change column name if both dataset has the same column name of columns\n",
    "# like: registar has 'name', 'id' and login has 'name', 'id' --> merge dataset has 'name', 'id_reg', 'id_log'\n",
    "\n",
    "# datetime\n",
    "euro_date = '10-12-2000'    # 10december2000\n",
    "pd.to_datetime(euro_date)   # gives 2000-10-12 means 12october2000 (makes it american datetime)\n",
    "pd.to_datetime(euro_date, dayfirst=True) # gives 2000-12-10 means 10december2000\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['column_name'].value_counts().plot(kind='pie',\n",
    "                                        figsize=(15,8),\n",
    "                                        autopct='%1.0f%%',\n",
    "                                        explode=[0.04, 0.04, 0.04, 0.04, 0.04], # write it as many as distinct elemts are\n",
    "                                        colors=['ping', 'tomato', 'cornflowerblue', 'orange', 'orchid'],\n",
    "                                        shadow=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('tables.xlsx') as writer:\n",
    "    t_mp_current_stock.to_excel(writer, sheet_name='current_stock', index=False)\n",
    "    t_mp_holidays.to_excel(writer, sheet_name='holidays', index=False)\n",
    "    t_mp_lead_time.to_excel(writer, sheet_name='lead_time', index=False)\n",
    "    t_mp_main_sales[:900000].to_excel(writer, sheet_name='main_sales', index=False)\n",
    "    t_mp_main_sales[900000:].to_excel(writer, sheet_name='main_sales_2', index=False)\n",
    "    t_mp_main_stock_daily[:900000].to_excel(writer, sheet_name='main_stock_daily', index=False)\n",
    "    t_mp_main_stock_daily[900000:1800000].to_excel(writer, sheet_name='main_stock_daily_2', index=False)\n",
    "    t_mp_main_stock_daily[1800000:].to_excel(writer, sheet_name='main_stock_daily_3', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date to MontlyDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DATE_MONTHLY\"] = df[\"DATE\"].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_creation(df, n_lags=3):\n",
    "    for col in df.columns:\n",
    "        for i in range(1, n_lags+1):\n",
    "            df[col+f'_LAG_{i}'] = df[col].shift(i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day period\n",
    "day = 3\n",
    "\n",
    "class FileOperation:\n",
    "    \"\"\"\n",
    "    This class helps to create files, load/dump created model, and also save figures.\n",
    "    \"\"\"\n",
    "    def _init_(self):\n",
    "        pass\n",
    "    \n",
    "    # At each 3-day period, files can be removed with usinf file_date_remove function.\n",
    "    def file_date_remove(self, file_name:str):\n",
    "        #file_list = gLob(f\"{file_name｝**）\n",
    "        #for file_name in file_list：\n",
    "        if os.stat(file_name).st_mtime <= time.time() - day * 86400:\n",
    "            os.remove(file_name)\n",
    "            print (f\"File removed. File Path: (file_name)\")\n",
    "    # Create folder with desired folder path.\n",
    "    def create_folder(self, folder_path: str):\n",
    "        Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def joblib_file_load(self, file_name:str):\n",
    "        return joblib.load(filename=file_name)\n",
    "    \n",
    "    def joblib_file_dump(self, data, file_name:str):\n",
    "        joblib.dump(data, filename=file_name)\n",
    "        \n",
    "    def figure_dump(self, fig, file_name:str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files creation\n",
    "file_operation = FileOperation()\n",
    "# Create folder for datasets to save them inside later\n",
    "datasets = file_operation.create_folder(f\"{folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MS SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "connection_string = f'mssql+pyodbc://{server_name}/{db_name}?driver=ODBC Driver 17 for SQL Server'\n",
    "engine = create_engine(connection_string, echo=False, fast_executemany=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT * FROM {table_name}\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Bidirectional, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModelling:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, epochs, batch_size, verbose):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def lstm_fit(self):\n",
    "        \n",
    "        self.model.fit(self.x_train, self.x_train,\n",
    "                       epochs=self.epochs,\n",
    "                       batch_size=self.batch_size,\n",
    "                       verbose=self.verbose,\n",
    "                       validation_data=(self.x_test, self.y_test))\n",
    "        return self.lstm_evaluate_model()\n",
    "        \n",
    "    def lstm_model(self, n_steps, n_features, add_lstm_units, dropout, lstm_units=32, predict_value_num=1,\n",
    "                activation_function='relu', loss='mean_squared_error', optimizer='adam', bidirectional=False):\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        if bidirectional:\n",
    "            self.model.add(LSTM(lstm_units, # the number of LSTM units in the hidden layer\n",
    "                        activation=activation_function, # activation function\n",
    "                        input_shape=(n_steps, n_features), # #of time steps and #of features: (X_train.shape[1], X_train.shape[2])\n",
    "                        dropout=dropout,\n",
    "                        return_sequences=True) # True -> many-to-many\n",
    "                    )\n",
    "        else:\n",
    "            self.model.add(Bidirectional(LSTM(lstm_units, # the number of LSTM units in the hidden layer\n",
    "                        activation=activation_function, # activation function\n",
    "                        input_shape=(n_steps, n_features), # #of time steps and #of features: (X_train.shape[1], X_train.shape[2])\n",
    "                        dropout=dropout,\n",
    "                        return_sequences=True) # True -> many-to-many\n",
    "                    ))\n",
    "            \n",
    "        if len(add_lstm_units) != 0:\n",
    "            for i in add_lstm_units:\n",
    "                self.model.add(LSTM(i,\n",
    "                            return_sequences=True))\n",
    "                if len(dropout) != 0:\n",
    "                    self.model.add(Dropout(dropout)) # dropping out units: this helps having a network capable of better generalization and less likely to overfit the training data.\n",
    "        self.model.add(Dense(predict_value_num))\n",
    "        self.model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        return self.lstm_fit()\n",
    "    \n",
    "    def lstm_evaluate_model(self):\n",
    "        # Returns MSE (Mean Squared Error)\n",
    "        test_score = self.model.evaluate(self.x_test, self.y_test)\n",
    "        print(\"LSTM Model Score: {:.2f}\".format(test_score))\n",
    "        # The loss is a measure of how well the model is able to predict the correct output, while the accuracy is a measure of how often the model is correct.\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df.columns.tolist() \n",
    "col.remove('Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(data=df,\n",
    "          target='Churn',\n",
    "          session_id=123,\n",
    "          fold_shuffle=True,\n",
    "          numeric_features=col,\n",
    "          imputation_type='iterative',\n",
    "          remove_multicollinearity=True,\n",
    "          multicollinearity_threshold=0.95,\n",
    "          fix_imbalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_tuned = tune_model(lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n",
    "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n",
    "\n",
    "    cv_scores = np.empty(5)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            early_stopping_rounds=100,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        preds = model.predict_proba(X_test)\n",
    "        cv_scores[idx] = log_loss(y_test, preds)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\n",
    "func = lambda trial: objective(trial, X, y)\n",
    "study.optimize(func, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_3.drop('Churn', axis=1)\n",
    "y = df_3['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = Pool(X, y, feature_names=list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        'random_strength': trial.suggest_int('random_strength', 0, 100),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        #     \"used_ram_limit\": \"3gb\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\n",
    "            \"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    gbm = cb.CatBoostClassifier(**param)\n",
    "\n",
    "    gbm.fit(X_train_cat, eval_set=[\n",
    "            (X_test, y_test)], verbose=0, early_stopping_rounds=50)\n",
    "\n",
    "    preds = gbm.predict(X_test)\n",
    "    pred_labels = np.rint(preds)\n",
    "    f1 = f1_score(y_test, pred_labels)\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of completed trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"\\tBest Score: {}\".format(trial.value))\n",
    "print(\"\\tBest Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'CrossEntropy',\n",
    "    'colsample_bylevel': 0.06108670760673487,\n",
    "    'depth': 10,\n",
    "    'boosting_type': 'Plain',\n",
    "    'random_strength': 51,\n",
    "    'learning_rate': 0.1698330158794727,\n",
    "    'bootstrap_type': 'MVS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**params, verbose = True)\n",
    "model.fit(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = accuracy_score(y_test, pred_labels)\n",
    "precision = precision_score(y_test, pred_labels)\n",
    "recall = recall_score(y_test, pred_labels)\n",
    "f1 = f1_score(y_test, pred_labels)\n",
    "df_result = pd.DataFrame()\n",
    "row = {'Model': 'CatBoostClassifier',\n",
    "        'Accuracy': round(accuracy,3),\n",
    "        'Precision': round(precision,3),\n",
    "        'Recall': round(recall,3),\n",
    "        'F1': round(f1,3),\n",
    "        'ModelParameters':model.get_all_params()\n",
    "    }\n",
    "\n",
    "df_result = df_result.append(row, ignore_index=True)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, xticks_rotation='vertical')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study, params=['depth', 'learning_rate', 'bootstrap_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_submission = clean_test_df[[\"BASE_CUSTOMER_ID\"]].rename(columns={\"BASE_CUSTOMER_ID\":\"Id\"})\n",
    "\n",
    "xgb_pred = pd.DataFrame()\n",
    "xgb_pred[\"Expected\"] = predictions_test_xgb[:,1]\n",
    "lgb_pred = pd.DataFrame()\n",
    "lgb_pred[\"Expected\"] = predictions_test_lgb[:,1]\n",
    "cat_pred = pd.DataFrame()\n",
    "cat_pred[\"Expected\"] = predictions_test_cat[:,1]\n",
    "\n",
    "blended_submission[\"Expected\"] = (lgb_pred[\"Expected\"] * 0.5 + xgb_pred[\"Expected\"] * 0.4 + cat_pred[\"Expected\"] * 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cfb22f7ad758691768dcc572bcc29c8b09a8a49fcfaf5a8015c4aba90c19555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
